{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp-models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yagmurdenizlii/nlp/blob/main/nlp_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "H8tGzp9QyhjN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjvYAoUqDUNp"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install nltk\n",
        "! pip install glove-python-binary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "import numpy as np\n",
        "import statistics\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import re  \n",
        "import pandas as pd \n",
        "from time import time \n",
        "from collections import defaultdict  \n",
        "\n",
        "import spacy \n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import multiprocessing\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "nltk.download ('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import WordNetLemmatizer  \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('omw-1.4')\n",
        "from glove import Corpus, Glove\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import torchtext\n"
      ],
      "metadata": {
        "id": "kLDgsfsAXk9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "artist_names = [    'Beyonce', \n",
        "            'BillieEilish', \n",
        "            'CharliePuth',\n",
        "            'ColdPlay', \n",
        "            'Drake', \n",
        "            'DuaLipa',\n",
        "            'EdSheeran',\n",
        "            'JustinBieber',\n",
        "            'KatyPerry',\n",
        "            'LadyGaga',\n",
        "            'Maroon5',\n",
        "            'NickiMinaj',\n",
        "            'Rihanna',\n",
        "            'SelenaGomez',\n",
        "            'TaylorSwift',\n",
        "            ]\n",
        "\n",
        "    \n",
        "list_with_spaces = ['Beyonce', \n",
        "                    'Billie Eilish', \n",
        "                    'Charlie Puth',\n",
        "                    'ColdPlay', \n",
        "                    'Drake', \n",
        "                    'Dua Lipa',\n",
        "                    'Ed Sheeran',\n",
        "                    'Justin Bieber',\n",
        "                    'Katy Perry',\n",
        "                    'Lady Gaga',\n",
        "                    'Maroon 5',\n",
        "                    'Nicki Minaj',\n",
        "                    'Rihanna',\n",
        "                    'Selena Gomez',\n",
        "                    'Taylor Swift',\n",
        "                    'beyoncé',\n",
        "                    'verse',\n",
        "                    'chris martin',\n",
        "                    'sean paul',\n",
        "                    'justin timberlake',\n",
        "                    'beyoncè'\n",
        "                    ]\n"
      ],
      "metadata": {
        "id": "mmi58QJGXsQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "list_of_files = [artist + '.csv' for artist in artist_names]\n",
        "\n",
        "all_lyrics = []\n",
        "\n",
        "for file_path in list_of_files:\n",
        "    df = pd.read_csv(file_path)\n",
        "    df = df.dropna()\n",
        "\n",
        "    text_list = df['Lyric'].values.tolist() \n",
        "\n",
        "    new_all_lyrics = []\n",
        "\n",
        "    for line in text_list:\n",
        "        new_line = line.replace(\"'\", '')\n",
        "        for name in list_with_spaces:\n",
        "            new_line = new_line.replace(name.lower(), '')\n",
        "        new_all_lyrics.append(new_line)\n",
        "\n",
        "    for row in new_all_lyrics:\n",
        "        if type(row) is str:\n",
        "            row_list = row.split()\n",
        "            nb_words = len(row_list)\n",
        "            if nb_words > 200:\n",
        "                nb_chunks = nb_words // 200 + 1\n",
        "                arr = np.array_split(row_list, nb_chunks)\n",
        "\n",
        "                for array in arr:\n",
        "                    all_lyrics.append(' '.join(array))\n",
        "\n",
        "            elif nb_words > 20:\n",
        "                all_lyrics.append(row)\n",
        "\n",
        "print('fin')       "
      ],
      "metadata": {
        "id": "vsKahYZWXs5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gets the tokenized list of words, returns the list after removing the stopwords\n",
        "def remove_stopwords(all):\n",
        "  all_words_final = []\n",
        "  for sentence in all:\n",
        "    sentence_final = []\n",
        "    for word in sentence:\n",
        "      if word not in stopwords.words('english'):\n",
        "        sentence_final.append(word)\n",
        "    all_words_final.append(sentence_final)\n",
        "\n",
        "  return all_words_final"
      ],
      "metadata": {
        "id": "Egje1xx7Xxq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_lemmatizer(all):\n",
        "  wordnet_lemmatizer = WordNetLemmatizer() \n",
        "\n",
        "  lemmatized = [] \n",
        "\n",
        "  for sentence in all: \n",
        "    lemmatized_words = []\n",
        "    for word in sentence:\n",
        "      lemmatized_words.append(wordnet_lemmatizer.lemmatize(word)) \n",
        "    lemmatized.append(lemmatized_words)\n",
        "\n",
        "  return lemmatized"
      ],
      "metadata": {
        "id": "86tkjP_yagsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#divide into two groups\n",
        "training_data, test_data = train_test_split(all_lyrics, test_size = 0.8, random_state=42)\n"
      ],
      "metadata": {
        "id": "MyE5bLzfdss9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section\n",
        "Word2Vec"
      ],
      "metadata": {
        "id": "aNyEesLMazGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize\n",
        "all_words = [nltk.word_tokenize(sent) for sent in training_data]"
      ],
      "metadata": {
        "id": "KlMT5ZXWbbB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove stopwords\n",
        "all_words_r = remove_stopwords(all_words)"
      ],
      "metadata": {
        "id": "bFTFU_crbenC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec = Word2Vec(all_words_r, min_count=1, hs=1, negative=0)"
      ],
      "metadata": {
        "id": "_Q22Paisa-3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training\n",
        "word2vec.train(all_words_r, total_examples = word2vec.corpus_count, epochs=10, report_delay=1)\n"
      ],
      "metadata": {
        "id": "AXlT1afKbRCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_tokens = [nltk.word_tokenize(sent) for sent in test_data]\n",
        "\n",
        "scores = word2vec.score(test_tokens, len(test_tokens))\n"
      ],
      "metadata": {
        "id": "OEW0aqLGQcrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pp = []\n",
        "i = 0\n",
        "\n",
        "for t in test_tokens:\n",
        "  pp.append(2 ** (scores[i] / len(t)))\n",
        "  i += 1\n",
        "\n",
        "print(pp)\n",
        "\n",
        "average = np.sum(pp) / len(pp)\n",
        "print(average)\n"
      ],
      "metadata": {
        "id": "IW_cKaaRROgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section\n",
        "Word2Vec 2"
      ],
      "metadata": {
        "id": "miM981PRceXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model = Word2Vec(min_count=20,\n",
        "                     window=2,\n",
        "                     size=300,\n",
        "                     sample=6e-5, \n",
        "                     alpha=0.03, \n",
        "                     min_alpha=0.0007, \n",
        "                     negative=20,\n",
        "                     workers=3)"
      ],
      "metadata": {
        "id": "WNMC20pVcuAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.build_vocab(all_words_r, progress_per=10000)"
      ],
      "metadata": {
        "id": "imPy8WhRc0cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.train(all_words_r, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
      ],
      "metadata": {
        "id": "I5_Yd32zc1IR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similar_words = w2v_model.wv.most_similar('love')\n",
        "w2v_model.wv.most_similar(positive=[\"love\", \"amazing\"], negative=[\"hate\"], topn=3)"
      ],
      "metadata": {
        "id": "SuAcL2T3dH4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model"
      ],
      "metadata": {
        "id": "nxq00AzUji7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "Z_O_IumfykEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training_data, test_data\n",
        "\n",
        "test_tokens = [nltk.word_tokenize(sent) for sent in test_data]\n",
        "test_tokens = my_lemmatizer(remove_stopwords(test_tokens))\n",
        "\n"
      ],
      "metadata": {
        "id": "rBnfhE1VghH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "unigram = {}\n",
        "for song in all_words_r:\n",
        "  for f in song:\n",
        "      try:\n",
        "          unigram[f] += 1\n",
        "      except KeyError:\n",
        "          unigram[f] = 1\n",
        "          continue\n",
        "\n",
        "print(unigram)\n",
        "\n",
        "N = float(sum(unigram.values()))\n",
        "for word in unigram:\n",
        "    unigram[word] = unigram[word]/N\n",
        "\n",
        "print(unigram)\n"
      ],
      "metadata": {
        "id": "xsOAlO2HghSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "eyddQItex_p2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import string"
      ],
      "metadata": {
        "id": "Orep_3h_yA1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "filename = \"alice.txt\"\n",
        "text = open(filename, 'r', encoding='utf-8').read()\n",
        "text = text.lower()\n",
        "text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n"
      ],
      "metadata": {
        "id": "up44i8MZ37XA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "\n",
        "#index = chars.index('z') + 1\n",
        "#chars = chars[:index]\n",
        "\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "\n",
        "n_chars = len(text)\n",
        "n_vocab = len(chars)\n",
        "print (\"Total Characters: \", n_chars)\n",
        "print (\"Total Vocab: \", n_vocab)\n",
        "\n"
      ],
      "metadata": {
        "id": "2l_9mbxM_9zB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = text[i:i + seq_length]\n",
        "\tseq_out = text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print (\"Total Patterns: \", n_patterns)"
      ],
      "metadata": {
        "id": "HBBIx1XQCVvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "...\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = to_categorical(dataY)"
      ],
      "metadata": {
        "id": "BeIvBBkhCqnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "86L9PdjeyIGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "metadata": {
        "id": "JHKgAzgxCzTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
      ],
      "metadata": {
        "id": "H20fqqx0C0Dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'weights-improvement-50-1.1209.hdf5'\n",
        "pre_model = load_model(filename)\n",
        "pre_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "metadata": {
        "id": "4sEOBuM1ylSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pick a random seed\n",
        "start = np.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(200):\n",
        "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = pre_model.predict(x, verbose=0)\n",
        "    index = np.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "    print(result, end =\"\")\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "metadata": {
        "id": "FTXsFpaGyO9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "oA_2JHNp7cZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "filename = \"alice.txt\"\n",
        "text = open(filename, 'r', encoding='utf-8').read()\n",
        "text = text.lower()\n",
        "text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "text = text.split()\n",
        "\n",
        "words = sorted(list(set(text)))\n",
        "\n",
        "print(words)\n",
        "\n",
        "#index = chars.index('z') + 1\n",
        "#chars = chars[:index]\n",
        "\n",
        "word_to_int = dict((c, i) for i, c in enumerate(words))\n",
        "\n",
        "n_words = len(text)\n",
        "n_vocab = len(words)\n",
        "print (\"Total words: \", n_words)\n",
        "print (\"Total Vocab: \", n_vocab)"
      ],
      "metadata": {
        "id": "glj0-hlg7dS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 50\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_words - seq_length, 1):\n",
        "\tseq_in = text[i:i + seq_length]\n",
        "\tseq_out = text[i + seq_length]\n",
        "\tdataX.append([word_to_int[word] for word in seq_in])\n",
        "\tdataY.append(word_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print (\"Total Patterns: \", n_patterns)"
      ],
      "metadata": {
        "id": "eLCNqEgF9oum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = to_categorical(dataY)\n",
        "\n",
        "# define the LSTM model\n",
        "w_model = Sequential()\n",
        "w_model.add(LSTM(512, input_shape=(X.shape[1], X.shape[2])))\n",
        "w_model.add(Dropout(0.2))\n",
        "w_model.add(Dense(y.shape[1], activation='softmax'))\n",
        "w_model.compile(loss='categorical_crossentropy', optimizer='adam')\n"
      ],
      "metadata": {
        "id": "12t3HDaS9h_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath=\"aalice-weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "metadata": {
        "id": "MsVkEgQu-BDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_model.fit(X, y, epochs=3, batch_size=128, callbacks=callbacks_list)"
      ],
      "metadata": {
        "id": "XpVOeTn3-HDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'aalice-weights-improvement-14-6.0146.hdf5'\n",
        "pre_model_alice = load_model(filename)\n",
        "pre_model_alice.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "int_to_word = dict((i, c) for i, c in enumerate(words))"
      ],
      "metadata": {
        "id": "6vs0g4e--rqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pick a random seed\n",
        "start = np.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ' '.join([int_to_word[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(100):\n",
        "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = pre_model_alice.predict(x, verbose=0)\n",
        "    index = np.argmax(prediction)\n",
        "    for i in range(random.randint(0, 2)):\n",
        "      prediction = np.delete(prediction, index)\n",
        "      index = np.argmax(prediction)\n",
        "    result = int_to_word[index]\n",
        "    print(result, end =\" \")\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "metadata": {
        "id": "4Q36qc0I_Pjb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}