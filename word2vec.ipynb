{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN8YgdP1fElucYjjG1IUC0i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yagmurdenizlii/NLP/blob/main/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPspmoAaXx-v"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install nltk\n",
        "!pip install glove-python-binary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import string\n",
        "import csv\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import torchtext\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from gensim.models import Word2Vec\n",
        "from glove import Corpus, Glove\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "nltk.download ('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import WordNetLemmatizer  \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ],
      "metadata": {
        "id": "ODQ6GFDfX7uH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gets the tokenized list of words, returns the list after removing the stopwords\n",
        "def remove_stopwords(all):\n",
        "  all_words_final = []\n",
        "  for sentence in all:\n",
        "    sentence_final = []\n",
        "    for word in sentence:\n",
        "      if word not in stopwords.words('english'):\n",
        "        sentence_final.append(word)\n",
        "    all_words_final.append(sentence_final)\n",
        "\n",
        "  return all_words_final"
      ],
      "metadata": {
        "id": "HbeLodW9YBo6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_lemmatizer(all):\n",
        "  wordnet_lemmatizer = WordNetLemmatizer() \n",
        "\n",
        "  lemmatized = [] \n",
        "\n",
        "  for sentence in all: \n",
        "    lemmatized_words = []\n",
        "    for word in sentence:\n",
        "      lemmatized_words.append(wordnet_lemmatizer.lemmatize(word)) \n",
        "    lemmatized.append(lemmatized_words)\n",
        "\n",
        "  return lemmatized"
      ],
      "metadata": {
        "id": "wR501_h5YDcq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_preprocessor(input):\n",
        "\n",
        "  text = ''\n",
        "\n",
        "  if isinstance(input, str):\n",
        "    text = input.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "  else:\n",
        "    text = []\n",
        "    for line in input:\n",
        "      tt = line.lower()\n",
        "      tt = tt.translate(str.maketrans('', '', string.punctuation))\n",
        "      if(len(line.split()) > 0):\n",
        "        text.append(tt)\n",
        "\n",
        "  return text\n"
      ],
      "metadata": {
        "id": "pxmSe7Q4YnN1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_w2v(training_data, remove_sw, lemmatize):\n",
        "\n",
        "  all_words = [nltk.word_tokenize(sent) for sent in training_data]\n",
        "\n",
        "  if remove_sw == True:\n",
        "    all_words = remove_stopwords(all_words)\n",
        "\n",
        "  if lemmatize == True:\n",
        "    all_words = my_lemmatizer(all_words)\n",
        "\n",
        "  word2vec = Word2Vec(all_words, min_count=2, hs=1, negative=0)\n",
        "\n",
        "  word2vec.train(all_words, total_examples = word2vec.corpus_count, epochs=20, report_delay=1)\n",
        "\n",
        "  return word2vec\n"
      ],
      "metadata": {
        "id": "9HDg6PkpZTfx"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_w2v_tests(test_data, word2vec, remove_sw, lemmatize):\n",
        "  test_tokens = [nltk.word_tokenize(sent) for sent in test_data]\n",
        "\n",
        "  if remove_sw == True:\n",
        "    test_tokens = remove_stopwords(test_tokens)\n",
        "\n",
        "  if lemmatize == True:\n",
        "    test_tokens = my_lemmatizer(test_tokens)\n",
        "\n",
        "  scores = word2vec.score(test_tokens, len(test_tokens))\n",
        "\n",
        "  pp = []\n",
        "  i = 0\n",
        "\n",
        "  for t in test_tokens:\n",
        "    pp.append(2 ** (scores[i] / len(t)))\n",
        "    i += 1\n",
        "\n",
        "  average = np.sum(pp) / len(pp)\n",
        "  return average"
      ],
      "metadata": {
        "id": "8oKRsQxtpkKQ"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(path, column_name):\n",
        "\n",
        "  type_of_file = path.split('.')[-1]\n",
        "\n",
        "  text = ''\n",
        "  list_of_text = []\n",
        "\n",
        "  if type_of_file == 'txt':\n",
        "    text = open(path, 'r', encoding='utf-8').read()\n",
        "    words = text.split()\n",
        "    np_strings = np.array(words)\n",
        "    list_of_text_np = np.array_split(np_strings, 100)\n",
        "\n",
        "    for list in list_of_text_np:\n",
        "      list_of_text.append(' '.join(list))\n",
        "      \n",
        "  elif type_of_file == 'csv':\n",
        "    df = pd.read_csv(path)\n",
        "    df = df.dropna()\n",
        "    list_of_text = df[column_name].values.tolist() \n",
        "  \n",
        "  else: \n",
        "    print('File type is not supported.')\n",
        "\n",
        "  return text, list_of_text"
      ],
      "metadata": {
        "id": "X3LvKWNYhNLM"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text, list_of_text = read_data('alice.txt', '')\n",
        "\n",
        "text = my_preprocessor(text)\n",
        "list_of_text = my_preprocessor(list_of_text)\n",
        "\n",
        "training_list, test_list = train_test_split(list_of_text, test_size = 0.9, random_state=42)\n",
        "\n",
        "w2v_model = my_w2v(training_list, lemmatize=True, remove_sw=False)\n",
        "wv_perplexity = my_w2v_tests(test_list, w2v_model, lemmatize=True, remove_sw=False)\n",
        "\n",
        "print(wv_perplexity)\n"
      ],
      "metadata": {
        "id": "oK23Ew8wodA9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}