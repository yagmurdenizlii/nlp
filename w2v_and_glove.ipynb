{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w2v-and-glove.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyORUlk1uK2ZGus4ZwS+j6J+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yagmurdenizlii/nlp/blob/main/w2v_and_glove.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install torchvision"
      ],
      "metadata": {
        "id": "gYruF7SAg1St"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "FuBcBMR5XNJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "import numpy as np\n",
        "import statistics\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "6XiyM9QHfbCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "artist_names = [    'Beyonce', \n",
        "            'BillieEilish', \n",
        "            'CharliePuth',\n",
        "            'ColdPlay', \n",
        "            'Drake', \n",
        "            'DuaLipa',\n",
        "            'EdSheeran',\n",
        "            'JustinBieber',\n",
        "            'KatyPerry',\n",
        "            'LadyGaga',\n",
        "            'Maroon5',\n",
        "            'NickiMinaj',\n",
        "            'Rihanna',\n",
        "            'SelenaGomez',\n",
        "            'TaylorSwift',\n",
        "            ]\n",
        "\n",
        "    \n",
        "list_with_spaces = ['Beyonce', \n",
        "                    'Billie Eilish', \n",
        "                    'Charlie Puth',\n",
        "                    'ColdPlay', \n",
        "                    'Drake', \n",
        "                    'Dua Lipa',\n",
        "                    'Ed Sheeran',\n",
        "                    'Justin Bieber',\n",
        "                    'Katy Perry',\n",
        "                    'Lady Gaga',\n",
        "                    'Maroon 5',\n",
        "                    'Nicki Minaj',\n",
        "                    'Rihanna',\n",
        "                    'Selena Gomez',\n",
        "                    'Taylor Swift',\n",
        "                    'beyoncé',\n",
        "                    'verse',\n",
        "                    'chris martin',\n",
        "                    'sean paul',\n",
        "                    'justin timberlake',\n",
        "                    'beyoncè'\n",
        "                    ]\n",
        "\n"
      ],
      "metadata": {
        "id": "fCHC1cQhff8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "63eYT_sNrDGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vzP_O8oivn6",
        "outputId": "cb177515-343d-4984-bf7a-c09dbea3755c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fin\n"
          ]
        }
      ],
      "source": [
        "\n",
        "list_of_files = [artist + '.csv' for artist in artist_names]\n",
        "\n",
        "all_lyrics = []\n",
        "\n",
        "for file_path in list_of_files:\n",
        "    df = pd.read_csv(file_path)\n",
        "    df = df.dropna()\n",
        "\n",
        "    text_list = df['Lyric'].values.tolist() \n",
        "\n",
        "    new_all_lyrics = []\n",
        "\n",
        "    for line in text_list:\n",
        "        new_line = line.replace(\"'\", '')\n",
        "        for name in list_with_spaces:\n",
        "            new_line = new_line.replace(name.lower(), '')\n",
        "        new_all_lyrics.append(new_line)\n",
        "\n",
        "    for row in new_all_lyrics:\n",
        "        if type(row) is str:\n",
        "            row_list = row.split()\n",
        "            nb_words = len(row_list)\n",
        "            if nb_words > 200:\n",
        "                nb_chunks = nb_words // 200 + 1\n",
        "                arr = np.array_split(row_list, nb_chunks)\n",
        "\n",
        "                for array in arr:\n",
        "                    all_lyrics.append(' '.join(array) + '\\n')\n",
        "\n",
        "            elif nb_words > 20:\n",
        "                all_lyrics.append(row)\n",
        "\n",
        "print('fin')       "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# dataset definition\n",
        "class MyDataset(Dataset):\n",
        "    # load the dataset\n",
        "    def __init__(self, list):\n",
        "        # store the inputs and outputs\n",
        "        self.list = list\n",
        "        \n",
        " \n",
        "    # number of rows in the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.list)\n",
        " \n",
        "    # get a row at an index\n",
        "    def __getitem__(self, idx):\n",
        "        return [self.list[idx]]"
      ],
      "metadata": {
        "id": "1cRpZLHleuOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data as data\n",
        "from torchvision import datasets\n",
        "\n",
        "# create the dataset\n",
        "dataset = MyDataset(all_lyrics)\n",
        "# select rows from the dataset\n",
        "train, test = data.random_split(dataset, [len(all_lyrics)*9//10, len(all_lyrics)//10 + 1])\n",
        "# create a data loader for train and test sets\n",
        "train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
        "test_dl = DataLoader(test, batch_size=1024, shuffle=False)"
      ],
      "metadata": {
        "id": "K0mT5E_NitDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re  \n",
        "import pandas as pd \n",
        "from time import time \n",
        "from collections import defaultdict  \n",
        "\n",
        "import spacy \n",
        "\n",
        "import logging  \n",
        "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
      ],
      "metadata": {
        "id": "YNHtF8vKpjis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_lines = []\n",
        "\n",
        "all_words = [nltk.word_tokenize(sent) for sent in all_lyrics]\n",
        "\n",
        "print(len(all_words))\n",
        "print(len(all_words[0]))\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "all_words_final = []\n",
        "for group in all_words:\n",
        "  group_final = []\n",
        "  for word in group:\n",
        "    if word not in stopwords.words('english'):\n",
        "      group_final.append(word)\n",
        "  all_words_final.append(group_final)\n",
        "\n",
        "print(len(all_words_final))\n",
        "print(len(all_words_final[0]))\n",
        "\n"
      ],
      "metadata": {
        "id": "VTjm9vHlSxtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "word2vec = Word2Vec(all_words, min_count=2)"
      ],
      "metadata": {
        "id": "IscsW9Znhwd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = word2vec.wv.vocab\n",
        "print(vocabulary)\n",
        "\n",
        "t = time()\n",
        "word2vec.train(all_words, total_examples=word2vec.corpus_count, epochs=30, report_delay=1)\n",
        "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
      ],
      "metadata": {
        "id": "7PKbDQm5iKJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similar_words = word2vec.wv.most_similar('love')\n",
        "similar_words"
      ],
      "metadata": {
        "id": "fhWwyJq7oPPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec.wv.most_similar(positive=[\"love\", \"amazing\"], negative=[\"hate\"], topn=3)\n"
      ],
      "metadata": {
        "id": "ljmSxbtAu4j_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "i-IfTYKVo3Mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model = Word2Vec(min_count=20,\n",
        "                     window=2,\n",
        "                     size=300,\n",
        "                     sample=6e-5, \n",
        "                     alpha=0.03, \n",
        "                     min_alpha=0.0007, \n",
        "                     negative=20,\n",
        "                     workers=3)\n",
        "\n",
        "\n",
        "t = time()\n",
        "w2v_model.build_vocab(all_words_final, progress_per=10000)\n",
        "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
        "\n",
        "t = time()\n",
        "w2v_model.train(all_words_final, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
        "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
      ],
      "metadata": {
        "id": "OKchRyJ0o4V7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.wv.most_similar(positive=[\"love\"])\n"
      ],
      "metadata": {
        "id": "M_Ye6e3tqiIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.wv.most_similar(positive=[\"love\", \"amazing\"], negative=[\"hate\"], topn=3)\n"
      ],
      "metadata": {
        "id": "VBd8WxporEjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "! pip install glove-python-binary\n",
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "nltk.download ('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "qPMgUBFZxsqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "all_tokens = []\n",
        "\n",
        "for song in all_lyrics:\n",
        " words = word_tokenize(song)\n",
        " all_tokens.append(words)"
      ],
      "metadata": {
        "id": "NlYHDXNSQim1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "all_tokens_final = []\n",
        "for group in all_tokens:\n",
        "  groups = []\n",
        "  for word in group:\n",
        "    if word not in stopwords.words('english'):\n",
        "      groups.append(word)\n",
        "  all_tokens_final.append(groups)\n",
        "\n",
        "print(len(all_tokens_final))\n",
        "print(len(all_tokens_final[0]))"
      ],
      "metadata": {
        "id": "S3EBTw2ER8i0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import WordNetLemmatizer  \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer() \n",
        "\n",
        "lemmatized = [] \n",
        "\n",
        "for sentence in all_tokens_final: \n",
        "  lemmatized_words = []\n",
        "  for word in sentence:\n",
        "    lemmatized_words.append(wordnet_lemmatizer.lemmatize(word)) \n",
        "  lemmatized.append(lemmatized_words)"
      ],
      "metadata": {
        "id": "p8-ZCaa5S4qV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet_lemmatizer.lemmatize('leaves')"
      ],
      "metadata": {
        "id": "7WLEKzghVo8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from glove import Corpus, Glove\n",
        "\n",
        "corpus = Corpus() \n",
        "\n",
        "\n",
        "corpus.fit(lemmatized, window=10)\n",
        "\n",
        "\n",
        "glove = Glove(no_components=5, learning_rate=0.05)\n",
        " \n",
        "glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n",
        "glove.add_dictionary(corpus.dictionary)\n",
        "glove.save('glove.model')"
      ],
      "metadata": {
        "id": "qIHIw-eeUrwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "love = glove.word_vectors[glove.dictionary['love']]\n",
        "hate =  glove.word_vectors[glove.dictionary['hate']]"
      ],
      "metadata": {
        "id": "uHSwvNKKb3ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "cosine_similarity([love], [hate])"
      ],
      "metadata": {
        "id": "LZxSJtA9cBkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext\n",
        "\n",
        "# The first time you run this will download a ~823MB file\n",
        "glove2 = torchtext.vocab.GloVe(name=\"6B\", dim=50)"
      ],
      "metadata": {
        "id": "ieq_CdzQdSJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = glove2['cat']\n",
        "y = glove2['dog']\n",
        "\n",
        "torch.norm(y - x)\n",
        "\n",
        "torch.cosine_similarity(x.unsqueeze(0), y.unsqueeze(0))"
      ],
      "metadata": {
        "id": "eJxo9-HshViz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = glove2['pop']\n",
        "b = glove2['music']\n",
        "\n",
        "torch.norm(a - b)\n",
        "\n",
        "torch.cosine_similarity(a.unsqueeze(0), b.unsqueeze(0))"
      ],
      "metadata": {
        "id": "a5zYtYJHhrXk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}